{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Operations in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before introducing PyTorch, we will first implement a simple network using numpy (which you should already be familiar with)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Search in the documentation whenever you feel unsure: https://pytorch.org/docs/stable/torch.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2.3520184568836067\n",
      "100 0.9333180231669476\n",
      "200 0.48612805469839365\n",
      "300 0.2803719033005763\n",
      "400 0.1744724200835167\n",
      "500 0.11463585591478825\n",
      "600 0.07832964286917948\n",
      "700 0.055092783833448344\n",
      "800 0.03961719982893863\n",
      "900 0.029004932094094965\n",
      "1000 0.021513919550906938\n",
      "1100 0.016119207864832934\n",
      "1200 0.012179067559047903\n",
      "1300 0.009267101210280654\n",
      "1400 0.007096674113319451\n",
      "1500 0.005464666405809898\n",
      "1600 0.0042290780438775325\n",
      "1700 0.0032866944944580155\n",
      "1800 0.0025635825342391573\n",
      "1900 0.0020062774976896554\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Simple feedforward network with ReLU activation.\n",
    "\n",
    "N: batch size,\n",
    "D_in: input dimension\n",
    "H: hidden dimension\n",
    "D_out: output dimension\n",
    "'''\n",
    "N, D_in, H, D_out = 64, 100, 200, 5\n",
    "\n",
    "# create random input and output data\n",
    "x = np.random.randn(N, D_in)*0.001 # input\n",
    "y = np.random.randn(N, D_out)*0.001 # output (i.e., labels)\n",
    "\n",
    "\n",
    "#Randomly initialize weights, bias terms are ignored\n",
    "w1 = np.random.randn(D_in, H)\n",
    "w2 = np.random.randn(H, D_out)\n",
    "\n",
    "learning_rate = 1e-1 # you can change this value\n",
    "# run for 2000 steps\n",
    "for t in range(2000):\n",
    "    # Forward pass: compute predicted y\n",
    "    h = x.dot(w1) # shape: (N, H)\n",
    "    h_relu = np.maximum(h, 0) # add relu activation\n",
    "    y_pred = h_relu.dot(w2) # shape: (N, D_out)\n",
    "    \n",
    "    # Compute and print loss\n",
    "    loss = np.square(y_pred, y).sum()\n",
    "    if t%100 == 0:\n",
    "        print (t, loss)\n",
    "    \n",
    "    # Backprop to compute gradients of w1 and w2 with respect to loss\n",
    "    grad_y_pred = 2.0*(y_pred - y) # shape: (N, D_out)\n",
    "    grad_w2 = h_relu.T.dot(grad_y_pred) # shape: (H, D_out)\n",
    "    grad_h_relu = grad_y_pred.dot(w2.T) # shape: (N, H)\n",
    "    grad_h = grad_h_relu.copy()\n",
    "    grad_h[h<0] = 0 # hidden state values < 0 have no grad due to ReLU\n",
    "    grad_w1 = x.T.dot(grad_h) # shape: (D_in, H)\n",
    "    \n",
    "    # Update weights\n",
    "    w1 -= learning_rate * grad_w1 \n",
    "    w2 -= learning_rate * grad_w2 \n",
    "    \n",
    "    \n",
    "## run this and you should see the loss decreasing     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpy is a great framework, but it cannot utilize GPUs to accelerate its numerical computations. For modern deep neural networks, GPUs often provide speedups of 50x or greater, so unfortunately numpy won’t be enough for modern deep learning. Also, it's kind of troublesome to write the gradient calculation and backprop manually every time, especially for complext networks. Fortunately with PyTorch, things will be much easier.\n",
    "\n",
    "Here we introduce the most fundamental PyTorch concept: the Tensor. A PyTorch Tensor is conceptually identical to a numpy array: a Tensor is an n-dimensional array, and PyTorch provides many functions for operating on these Tensors. Behind the scenes, Tensors can keep track of a computational graph and gradients, but they’re also useful as a generic tool for scientific computing.\n",
    "\n",
    "Also unlike numpy, PyTorch Tensors can utilize GPUs to accelerate their numeric computations. To run a PyTorch Tensor on GPU, you simply need to cast it to a new datatype.\n",
    "\n",
    "Here we first introduce some basic operations for PyTorch tensors, you will find them similar to NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "define a helper function that will summarise various properties of a tensor:\n",
    "type, dimension, and contents of the tensor.\n",
    "'''\n",
    "def describe(x):\n",
    "    print (\"Type: {}\".format(x.type()))\n",
    "    print (\"Shape/Size: {}\".format(x.shape))\n",
    "    print (\"Values: \\n{}\".format(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5.1\n",
      "Type: torch.FloatTensor\n",
      "Shape/Size: torch.Size([2, 3])\n",
      "Values: \n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.manual_seed(2020) # set a random seed for reproducing\n",
    "print (torch.__version__)\n",
    "\n",
    "# randomly initialize a tensor by specifying dimensions\n",
    "describe(torch.Tensor(2, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: torch.FloatTensor\n",
      "Shape/Size: torch.Size([2, 3])\n",
      "Values: \n",
      "tensor([[0.4869, 0.1052, 0.5883],\n",
      "        [0.1161, 0.4949, 0.2824]])\n",
      "Type: torch.FloatTensor\n",
      "Shape/Size: torch.Size([2, 3])\n",
      "Values: \n",
      "tensor([[-0.0264, -0.1360, -0.3136],\n",
      "        [ 0.6418,  1.1961,  0.9936]])\n"
     ]
    }
   ],
   "source": [
    "describe(torch.rand(2, 3)) # uniform distribution [0, 1)\n",
    "describe(torch.randn(2, 3)) # normal distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also create tensors all filled with the same scalar. For creating a tensor of zeros or ones, we have built-in functions, and for filling it with specific values, we can use the fill_() method. Any PyTorch method with an underscore refers to an in-place operation; that is, it modifies the content in place without creating a new object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: torch.FloatTensor\n",
      "Shape/Size: torch.Size([2, 3])\n",
      "Values: \n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "Type: torch.FloatTensor\n",
      "Shape/Size: torch.Size([2, 3])\n",
      "Values: \n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "Type: torch.FloatTensor\n",
      "Shape/Size: torch.Size([2, 3])\n",
      "Values: \n",
      "tensor([[5., 5., 5.],\n",
      "        [5., 5., 5.]])\n"
     ]
    }
   ],
   "source": [
    "describe(torch.zeros(2, 3))\n",
    "x = torch.ones(2, 3)\n",
    "describe(x)\n",
    "x.fill_(5)\n",
    "describe(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also convert between NumPy arrays and PyTorch tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: torch.DoubleTensor\n",
      "Shape/Size: torch.Size([2, 3])\n",
      "Values: \n",
      "tensor([[0.9477, 0.8570, 0.2478],\n",
      "        [0.5205, 0.9915, 0.0510]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "npy = np.random.rand(2, 3)\n",
    "describe(torch.from_numpy(npy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the type of the tensor is DoubleTensor instead of the default FloatTensor. This corresponds with the data type of the NumPy random matrix, a float64.\n",
    "\n",
    "You can also convert a tensor to other types (float, double, long, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: torch.LongTensor\n",
      "Shape/Size: torch.Size([2, 3])\n",
      "Values: \n",
      "tensor([[5, 5, 5],\n",
      "        [5, 5, 5]])\n"
     ]
    }
   ],
   "source": [
    "x = x.long() # convert to long type\n",
    "describe(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you have created tensors, you can operate on them just like you do for Python or NumPy, or use PyTorch functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: torch.FloatTensor\n",
      "Shape/Size: torch.Size([2, 3])\n",
      "Values: \n",
      "tensor([[ 1.0911, -0.5916, -3.1703],\n",
      "        [-0.0083, -1.0251,  0.7644]])\n",
      "Type: torch.FloatTensor\n",
      "Shape/Size: torch.Size([2, 3])\n",
      "Values: \n",
      "tensor([[ 2.1821, -1.1832, -6.3406],\n",
      "        [-0.0165, -2.0501,  1.5288]])\n",
      "Type: torch.FloatTensor\n",
      "Shape/Size: torch.Size([2, 3])\n",
      "Values: \n",
      "tensor([[ 2.1821, -1.1832, -6.3406],\n",
      "        [-0.0165, -2.0501,  1.5288]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(2, 3)\n",
    "describe(a)\n",
    "describe(a+a)\n",
    "describe(torch.add(a, a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keeping track of the shape of your tensors is important when coding models. You can use view() function to reshape them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: torch.LongTensor\n",
      "Shape/Size: torch.Size([6])\n",
      "Values: \n",
      "tensor([0, 1, 2, 3, 4, 5])\n",
      "Type: torch.LongTensor\n",
      "Shape/Size: torch.Size([2, 3])\n",
      "Values: \n",
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.arange(6)\n",
    "describe(a)\n",
    "\n",
    "a = a.view(2, 3)\n",
    "describe(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can specify the dimension in which you want to perform certain operations, like sum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: torch.LongTensor\n",
      "Shape/Size: torch.Size([3])\n",
      "Values: \n",
      "tensor([3, 5, 7])\n",
      "Type: torch.LongTensor\n",
      "Shape/Size: torch.Size([2])\n",
      "Values: \n",
      "tensor([ 3, 12])\n",
      "Type: torch.LongTensor\n",
      "Shape/Size: torch.Size([3, 2])\n",
      "Values: \n",
      "tensor([[0, 3],\n",
      "        [1, 4],\n",
      "        [2, 5]])\n"
     ]
    }
   ],
   "source": [
    "describe(torch.sum(a, dim=0)) # sum along the first dimension\n",
    "describe(torch.sum(a, dim=1)) # sum along the second dimension\n",
    "describe(torch.transpose(a, 0, 1)) # swap the dimensions (2,3)->(3,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can do indexing, slicing and joining in NumPy style as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: torch.LongTensor\n",
      "Shape/Size: torch.Size([2, 3])\n",
      "Values: \n",
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5]])\n",
      "Type: torch.LongTensor\n",
      "Shape/Size: torch.Size([1, 2])\n",
      "Values: \n",
      "tensor([[0, 1]])\n",
      "Type: torch.LongTensor\n",
      "Shape/Size: torch.Size([])\n",
      "Values: \n",
      "1\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(6).view(2, 3)\n",
    "describe(x)\n",
    "describe(x[:1, :2])\n",
    "describe(x[0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use PyTorch indexing functions for complex indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: torch.LongTensor\n",
      "Shape/Size: torch.Size([2, 2])\n",
      "Values: \n",
      "tensor([[0, 2],\n",
      "        [3, 5]])\n"
     ]
    }
   ],
   "source": [
    "indices = torch.LongTensor([0, 2])\n",
    "describe(torch.index_select(x, dim=1, index=indices))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can join tensors with the concatenation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: torch.LongTensor\n",
      "Shape/Size: torch.Size([2, 3])\n",
      "Values: \n",
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5]])\n",
      "Type: torch.LongTensor\n",
      "Shape/Size: torch.Size([4, 3])\n",
      "Values: \n",
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5],\n",
      "        [0, 1, 2],\n",
      "        [3, 4, 5]])\n",
      "Type: torch.LongTensor\n",
      "Shape/Size: torch.Size([2, 6])\n",
      "Values: \n",
      "tensor([[0, 1, 2, 0, 1, 2],\n",
      "        [3, 4, 5, 3, 4, 5]])\n",
      "Type: torch.LongTensor\n",
      "Shape/Size: torch.Size([2, 2, 3])\n",
      "Values: \n",
      "tensor([[[0, 1, 2],\n",
      "         [3, 4, 5]],\n",
      "\n",
      "        [[0, 1, 2],\n",
      "         [3, 4, 5]]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(6).view(2, 3)\n",
    "describe(x)\n",
    "describe(torch.cat([x,x], dim=0))\n",
    "describe(torch.cat([x,x], dim=1))\n",
    "describe(torch.stack([x,x])) # simply stack 2 tensors together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do various numerical and linear algebra operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: torch.LongTensor\n",
      "Shape/Size: torch.Size([2, 3])\n",
      "Values: \n",
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5]])\n",
      "Type: torch.FloatTensor\n",
      "Shape/Size: torch.Size([3, 2])\n",
      "Values: \n",
      "tensor([[1., 2.],\n",
      "        [1., 2.],\n",
      "        [1., 2.]])\n"
     ]
    }
   ],
   "source": [
    "x1 = torch.arange(6).view(2, 3)\n",
    "describe(x1)\n",
    "x2 = torch.ones(3, 2)\n",
    "x2[:, 1] += 1 # broadcasting\n",
    "describe(x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: torch.LongTensor\n",
      "Shape/Size: torch.Size([2, 2])\n",
      "Values: \n",
      "tensor([[ 3,  6],\n",
      "        [12, 24]])\n",
      "Type: torch.LongTensor\n",
      "Shape/Size: torch.Size([2, 2])\n",
      "Values: \n",
      "tensor([[ 3,  6],\n",
      "        [12, 24]])\n"
     ]
    }
   ],
   "source": [
    "# matrix multiplication\n",
    "# x1 and x2 need to be the same type\n",
    "describe(torch.mm(x1, x2.long()))\n",
    "describe(torch.matmul(x1, x2.long()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch tensor class encapsulates the data (the tensor itself) and a range of operations, such as algebraic operations, indexing, and reshaping operations. \n",
    "\n",
    "More importantly, we can use automatic differentiation to automate the computation of backward passes in neural networks. The autograd package in PyTorch provides exactly this functionality. When using autograd, the forward pass of your network will define a computational graph; nodes in the graph will be Tensors, and edges will be functions that produce output Tensors from input Tensors. Backpropagating through this graph then allows you to easily compute gradients.\n",
    "\n",
    "This sounds complicated, it’s pretty simple to use in practice. Each Tensor represents a node in a computational graph. If x is a Tensor that has x.requires_grad=True then x.grad is another Tensor holding the gradient of x with respect to some scalar value.\n",
    "\n",
    "Here we use PyTorch Tensors and autograd to implement our two-layer network; now we no longer need to manually implement the backward pass through the network, and simply use loss.backward() to do the backprop!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 4.825924873352051\n",
      "100 0.9016178250312805\n",
      "200 0.46214866638183594\n",
      "300 0.26687490940093994\n",
      "400 0.1663445234298706\n",
      "500 0.10926847904920578\n",
      "600 0.07445896416902542\n",
      "700 0.05210120975971222\n",
      "800 0.037226129323244095\n",
      "900 0.027064412832260132\n",
      "1000 0.019950421527028084\n",
      "1100 0.014880144037306309\n",
      "1200 0.011212642304599285\n",
      "1300 0.008522626012563705\n",
      "1400 0.006527079734951258\n",
      "1500 0.005031653214246035\n",
      "1600 0.0039027994498610497\n",
      "1700 0.0030427188612520695\n",
      "1800 0.002383644925430417\n",
      "1900 0.0018749602604657412\n"
     ]
    }
   ],
   "source": [
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "# device = torch.device(\"cuda\") ## in case you run on GPU\n",
    "\n",
    "N, D_in, H, D_out = 64, 100, 200, 5\n",
    "\n",
    "# create random tensors to hold input and outputs\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)*0.001\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)*0.001\n",
    "\n",
    "# create random tensors for weights\n",
    "# remember to add requires_grad=True to indicate that we want to compute gradients with\n",
    "# respect to these Tensors during the backward pass.\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "\n",
    "learning_rate = 1e-1\n",
    "for t in range(2000):\n",
    "    # clamp function implements the ReLU activation\n",
    "    y_pred = x.mm(w1).clamp(min=0.).mm(w2)\n",
    "    \n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    if t%100 == 0:\n",
    "        print (t, loss.item())\n",
    "    \n",
    "    # Use autograd to compute the backward pass. This call will compute the\n",
    "    # gradient of loss with respect to all Tensors with requires_grad=True.\n",
    "    # After this call w1.grad and w2.grad will be Tensors holding the gradient\n",
    "    # of the loss with respect to w1 and w2 respectively.\n",
    "    loss.backward()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "        \n",
    "        # manually zero the gradients after updating weights\n",
    "        # otherwise they will accumulate\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "- Tensor and its operations\n",
    "\n",
    "- Use Autograd to do backprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "\n",
    "Natural Language Processing with PyTorch. O'REALLY. Delip Rao & Brian McMahan.\n",
    "\n",
    "Official PyTorch Tutorials: https://pytorch.org/tutorials/beginner/pytorch_with_examples.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
